{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61704a6809aa417f989024d6f1862302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de0f74309ede4e22a534d39519733cd9",
              "IPY_MODEL_8f0790f6b8924878933ccf5137d64d1b",
              "IPY_MODEL_4f6c4c3a3a404f3693c08d72b5a11f84"
            ],
            "layout": "IPY_MODEL_749f38db8f0a4674ba484d67b284186f"
          }
        },
        "de0f74309ede4e22a534d39519733cd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07d112edcbd7460babffaab9301fbf9f",
            "placeholder": "​",
            "style": "IPY_MODEL_6137a8ff3e094656ad63257765e5f55a",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8f0790f6b8924878933ccf5137d64d1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_903b907feb1545f4967d605d1e9e0a22",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_312df1e1776d483e8df8f2a68f8afbca",
            "value": 2
          }
        },
        "4f6c4c3a3a404f3693c08d72b5a11f84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_519c054f323043c593d8962903c14b32",
            "placeholder": "​",
            "style": "IPY_MODEL_aaff9cea70f94ee2a76b38602f6049b0",
            "value": " 2/2 [00:17&lt;00:00,  7.70s/it]"
          }
        },
        "749f38db8f0a4674ba484d67b284186f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07d112edcbd7460babffaab9301fbf9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6137a8ff3e094656ad63257765e5f55a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "903b907feb1545f4967d605d1e9e0a22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "312df1e1776d483e8df8f2a68f8afbca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "519c054f323043c593d8962903c14b32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaff9cea70f94ee2a76b38602f6049b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0b999daef73466eae106957d1648d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_804b771fdda5485ab2cfa5eea7763e73",
              "IPY_MODEL_c00ff57fcf3e4d7d918787afdca63bc2",
              "IPY_MODEL_acecc9fdee7d4f0983df21ac91ebebc4"
            ],
            "layout": "IPY_MODEL_2f16f423fd9546279c9b3502f090f5dd"
          }
        },
        "804b771fdda5485ab2cfa5eea7763e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b555086f4d6248628862092b82cde545",
            "placeholder": "​",
            "style": "IPY_MODEL_f67a3596fc134fa989c003ae5a243570",
            "value": "Map: 100%"
          }
        },
        "c00ff57fcf3e4d7d918787afdca63bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cf524b673354ae09bd1779629b2d0f3",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e8034b656394c4d90e8582b34c01cc7",
            "value": 100
          }
        },
        "acecc9fdee7d4f0983df21ac91ebebc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef2da19e62e24ada938ffc52e7009900",
            "placeholder": "​",
            "style": "IPY_MODEL_d572c425b3d6416f9d6bd99509ffa9cf",
            "value": " 100/100 [00:00&lt;00:00, 1646.97 examples/s]"
          }
        },
        "2f16f423fd9546279c9b3502f090f5dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b555086f4d6248628862092b82cde545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f67a3596fc134fa989c003ae5a243570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cf524b673354ae09bd1779629b2d0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e8034b656394c4d90e8582b34c01cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef2da19e62e24ada938ffc52e7009900": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d572c425b3d6416f9d6bd99509ffa9cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfR7FVcNdcHI",
        "outputId": "c0a1743a-6e6b-4694-a34d-3f45bfd0fb20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.6)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (1.1.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U bitsandbytes\n",
        "!!pip install -q -U accelerate\n",
        "!pip install peft\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, Trainer, TrainingArguments\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\n",
        "from datasets import load_dataset\n",
        "import time\n",
        "import psutil\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "# Define model and device\n",
        "model_path = \"microsoft/phi-2\"  # Updated model path\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Quantization configuration\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load model and tokenizer (without `.to(device)` for quantized models)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path, model_max_length=512, padding_side=\"left\", add_eos_token=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load SNLI dataset with specific sampling for NLI task\n",
        "snli_dataset = load_dataset('snli')\n",
        "train_dataset = snli_dataset['train'].select([i for i in range(0, 550000, 550)][:1000])\n",
        "val_dataset = snli_dataset['validation'].select([i for i in range(0, 10000, 100)][:100])\n",
        "test_dataset = snli_dataset['test'].select([i for i in range(0, 10000, 100)][:100])\n",
        "\n",
        "# Define tokenize functions - returning dict to avoid list concatenation issue\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch['premise'], batch['hypothesis'], truncation=True, max_length=512, padding=\"max_length\")\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True)\n",
        "\n",
        "# Improved evaluation function using regex and accuracy_score\n",
        "def evaluate_model(model, tokenizer, dataset, max_length=70):\n",
        "    print(f\"Dataset length: {len(dataset)}\")\n",
        "    model.eval()\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "    label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "\n",
        "    for i in tqdm(range(len(dataset)), position=0, leave=True, desc=\"Predicting on test Samples\"):\n",
        "        premise = dataset['premise'][i]\n",
        "        hypothesis = dataset['hypothesis'][i]\n",
        "        label = dataset['label'][i]\n",
        "\n",
        "        # Skip ambiguous label (-1) in SNLI dataset\n",
        "        if label == -1:\n",
        "            print(f\"Skipped example {i} (ambiguous label)\")\n",
        "            continue\n",
        "        true_labels.append(label_map[label])\n",
        "\n",
        "        # Concatenate premise and hypothesis with a specific prompt\n",
        "        input_text = (\n",
        "            f\"Premise: {premise}\\n\"\n",
        "            f\"Hypothesis: {hypothesis}\\n\"\n",
        "            f\"Answer with one of the following: entailment, neutral, contradiction.\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "        # Tokenize input text\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "        # Generate prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=4)  # Strict limit on max_new_tokens to get concise answers\n",
        "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "            # Use regex to extract the label directly after \"Answer:\"\n",
        "            match = re.search(r\"Answer:\\s*(entailment|neutral|contradiction)\", prediction, re.IGNORECASE)\n",
        "            if match:\n",
        "                prediction = match.group(1).lower()\n",
        "            else:\n",
        "                prediction = \"neutral\"  # Default if no match is found\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy, predictions\n",
        "\n",
        "# Ensure pad_token_id is set\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Evaluate the pre-trained model on the test set\n",
        "print(\"Evaluating pre-trained model on the test set...\")\n",
        "pretrained_accuracy, predictions = evaluate_model(model, tokenizer, test_dataset)\n",
        "\n",
        "# Display accuracy and sample predictions\n",
        "print(f\"\\nPre-trained Model Accuracy: {pretrained_accuracy * 100:.2f}%\")\n",
        "for i, prediction in enumerate(predictions[:5]):\n",
        "    print(f\"Example {i + 1}: {prediction}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "61704a6809aa417f989024d6f1862302",
            "de0f74309ede4e22a534d39519733cd9",
            "8f0790f6b8924878933ccf5137d64d1b",
            "4f6c4c3a3a404f3693c08d72b5a11f84",
            "749f38db8f0a4674ba484d67b284186f",
            "07d112edcbd7460babffaab9301fbf9f",
            "6137a8ff3e094656ad63257765e5f55a",
            "903b907feb1545f4967d605d1e9e0a22",
            "312df1e1776d483e8df8f2a68f8afbca",
            "519c054f323043c593d8962903c14b32",
            "aaff9cea70f94ee2a76b38602f6049b0",
            "b0b999daef73466eae106957d1648d48",
            "804b771fdda5485ab2cfa5eea7763e73",
            "c00ff57fcf3e4d7d918787afdca63bc2",
            "acecc9fdee7d4f0983df21ac91ebebc4",
            "2f16f423fd9546279c9b3502f090f5dd",
            "b555086f4d6248628862092b82cde545",
            "f67a3596fc134fa989c003ae5a243570",
            "1cf524b673354ae09bd1779629b2d0f3",
            "8e8034b656394c4d90e8582b34c01cc7",
            "ef2da19e62e24ada938ffc52e7009900",
            "d572c425b3d6416f9d6bd99509ffa9cf"
          ]
        },
        "id": "tX9OG2TxdhiA",
        "outputId": "11892330-c340-4ee3-e509-1dc7b0d48e38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61704a6809aa417f989024d6f1862302"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0b999daef73466eae106957d1648d48"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating pre-trained model on the test set...\n",
            "Dataset length: 100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting on test Samples:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   1%|          | 1/100 [00:00<00:58,  1.71it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   2%|▏         | 2/100 [00:01<00:50,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   3%|▎         | 3/100 [00:01<00:48,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   4%|▍         | 4/100 [00:02<00:49,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   5%|▌         | 5/100 [00:02<00:50,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   6%|▌         | 6/100 [00:03<00:49,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   7%|▋         | 7/100 [00:03<00:50,  1.84it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   8%|▊         | 8/100 [00:04<00:50,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:   9%|▉         | 9/100 [00:04<00:49,  1.83it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  10%|█         | 10/100 [00:05<00:47,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  11%|█         | 11/100 [00:05<00:45,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  12%|█▏        | 12/100 [00:06<00:44,  1.99it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  13%|█▎        | 13/100 [00:06<00:44,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  14%|█▍        | 14/100 [00:07<00:43,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  15%|█▌        | 15/100 [00:07<00:42,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  16%|█▌        | 16/100 [00:08<00:41,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  17%|█▋        | 17/100 [00:08<00:40,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  18%|█▊        | 18/100 [00:09<00:39,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  19%|█▉        | 19/100 [00:09<00:39,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  20%|██        | 20/100 [00:10<00:39,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  21%|██        | 21/100 [00:10<00:38,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  22%|██▏       | 22/100 [00:11<00:37,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  23%|██▎       | 23/100 [00:11<00:37,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  24%|██▍       | 24/100 [00:12<00:36,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  25%|██▌       | 25/100 [00:12<00:35,  2.10it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  26%|██▌       | 26/100 [00:13<00:36,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  27%|██▋       | 27/100 [00:13<00:35,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  28%|██▊       | 28/100 [00:14<00:34,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  29%|██▉       | 29/100 [00:14<00:34,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  30%|███       | 30/100 [00:15<00:35,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  31%|███       | 31/100 [00:15<00:35,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  32%|███▏      | 32/100 [00:16<00:37,  1.81it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  33%|███▎      | 33/100 [00:16<00:37,  1.79it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  34%|███▍      | 34/100 [00:17<00:37,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  35%|███▌      | 35/100 [00:17<00:34,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  36%|███▌      | 36/100 [00:18<00:33,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  37%|███▋      | 37/100 [00:18<00:32,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  38%|███▊      | 38/100 [00:19<00:30,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  39%|███▉      | 39/100 [00:19<00:31,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  40%|████      | 40/100 [00:20<00:31,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  41%|████      | 41/100 [00:20<00:30,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  42%|████▏     | 42/100 [00:21<00:28,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  43%|████▎     | 43/100 [00:21<00:28,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  44%|████▍     | 44/100 [00:22<00:27,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  45%|████▌     | 45/100 [00:22<00:26,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  46%|████▌     | 46/100 [00:23<00:26,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  47%|████▋     | 47/100 [00:23<00:25,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  48%|████▊     | 48/100 [00:24<00:24,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  49%|████▉     | 49/100 [00:24<00:24,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  50%|█████     | 50/100 [00:25<00:24,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  51%|█████     | 51/100 [00:25<00:23,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  52%|█████▏    | 52/100 [00:26<00:23,  2.08it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  53%|█████▎    | 53/100 [00:26<00:22,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  54%|█████▍    | 54/100 [00:27<00:22,  2.09it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  55%|█████▌    | 55/100 [00:27<00:22,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  56%|█████▌    | 56/100 [00:28<00:22,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  57%|█████▋    | 57/100 [00:28<00:23,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  58%|█████▊    | 58/100 [00:29<00:23,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  59%|█████▉    | 59/100 [00:30<00:23,  1.77it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  60%|██████    | 60/100 [00:30<00:21,  1.86it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  61%|██████    | 61/100 [00:31<00:20,  1.90it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  62%|██████▏   | 62/100 [00:31<00:19,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  63%|██████▎   | 63/100 [00:32<00:19,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  64%|██████▍   | 64/100 [00:32<00:18,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  65%|██████▌   | 65/100 [00:33<00:17,  1.98it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  66%|██████▌   | 66/100 [00:33<00:16,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  67%|██████▋   | 67/100 [00:33<00:16,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  68%|██████▊   | 68/100 [00:34<00:15,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  69%|██████▉   | 69/100 [00:34<00:15,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  70%|███████   | 70/100 [00:35<00:14,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  71%|███████   | 71/100 [00:35<00:14,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  72%|███████▏  | 72/100 [00:36<00:13,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  73%|███████▎  | 73/100 [00:36<00:13,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  74%|███████▍  | 74/100 [00:37<00:12,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  75%|███████▌  | 75/100 [00:37<00:12,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  76%|███████▌  | 76/100 [00:38<00:11,  2.04it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  77%|███████▋  | 77/100 [00:38<00:11,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  78%|███████▊  | 78/100 [00:39<00:10,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  79%|███████▉  | 79/100 [00:39<00:10,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  80%|████████  | 80/100 [00:40<00:09,  2.01it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  81%|████████  | 81/100 [00:40<00:09,  1.95it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  82%|████████▏ | 82/100 [00:41<00:09,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  83%|████████▎ | 83/100 [00:42<00:09,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  84%|████████▍ | 84/100 [00:42<00:09,  1.74it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  85%|████████▌ | 85/100 [00:43<00:08,  1.73it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  86%|████████▌ | 86/100 [00:43<00:07,  1.82it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  87%|████████▋ | 87/100 [00:44<00:06,  1.91it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  88%|████████▊ | 88/100 [00:44<00:06,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  89%|████████▉ | 89/100 [00:45<00:05,  1.89it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  90%|█████████ | 90/100 [00:45<00:05,  1.94it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  91%|█████████ | 91/100 [00:46<00:04,  1.96it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  92%|█████████▏| 92/100 [00:46<00:04,  2.00it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  93%|█████████▎| 93/100 [00:47<00:03,  2.03it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  94%|█████████▍| 94/100 [00:47<00:02,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  95%|█████████▌| 95/100 [00:48<00:02,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  96%|█████████▌| 96/100 [00:48<00:01,  2.05it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  97%|█████████▋| 97/100 [00:49<00:01,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  98%|█████████▊| 98/100 [00:49<00:00,  2.07it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples:  99%|█████████▉| 99/100 [00:50<00:00,  2.06it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Predicting on test Samples: 100%|██████████| 100/100 [00:50<00:00,  1.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pre-trained Model Accuracy: 50.00%\n",
            "Example 1: contradiction\n",
            "Example 2: entailment\n",
            "Example 3: entailment\n",
            "Example 4: contradiction\n",
            "Example 5: entailment\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display enhanced examples with premise, hypothesis, true label, and model prediction\n",
        "print(f\"\\nPre-trained Model Accuracy: {pretrained_accuracy * 100:.2f}%\")\n",
        "print(\"\\nDetailed Example Outputs:\")\n",
        "\n",
        "for i in range(10):  # Displaying 10 examples for more detailed inspection\n",
        "    premise = test_dataset['premise'][i]\n",
        "    hypothesis = test_dataset['hypothesis'][i]\n",
        "    true_label = label_map[test_dataset['label'][i]]\n",
        "    predicted_label = predictions[i]\n",
        "\n",
        "    print(f\"\\nExample {i + 1}:\")\n",
        "    print(f\"Premise: {premise}\")\n",
        "    print(f\"Hypothesis: {hypothesis}\")\n",
        "    print(f\"True Label: {true_label}\")\n",
        "    print(f\"Model Prediction: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbUy-E2siNO-",
        "outputId": "96e06fda-7e50-469b-8d83-289ef57e1ec9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pre-trained Model Accuracy: 50.00%\n",
            "\n",
            "Detailed Example Outputs:\n",
            "\n",
            "Example 1:\n",
            "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
            "Hypothesis: The church has cracks in the ceiling.\n",
            "True Label: neutral\n",
            "Model Prediction: contradiction\n",
            "\n",
            "Example 2:\n",
            "Premise: A woman within an orchestra is playing a violin.\n",
            "Hypothesis: A woman is playing the violin.\n",
            "True Label: entailment\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 3:\n",
            "Premise: Two men climbing on a wooden scaffold.\n",
            "Hypothesis: Two sad men climbing on a wooden scaffold.\n",
            "True Label: neutral\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 4:\n",
            "Premise: A man in a black shirt, in a commercial kitchen, holding up meat he took out of a bag.\n",
            "Hypothesis: A man in a black shirt, in a commercial kitchen, holding up the old meat he took out of a bag.\n",
            "True Label: neutral\n",
            "Model Prediction: contradiction\n",
            "\n",
            "Example 5:\n",
            "Premise: a woman in a black shirt looking at a bicycle.\n",
            "Hypothesis: A woman dressed in black shops for a bicycle.\n",
            "True Label: neutral\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 6:\n",
            "Premise: many children play in the water.\n",
            "Hypothesis: The children are playing mini golf.\n",
            "True Label: contradiction\n",
            "Model Prediction: contradiction\n",
            "\n",
            "Example 7:\n",
            "Premise: A group of people stand near and on a large black square on the ground with some yellow writing on it.\n",
            "Hypothesis: a group of people wait\n",
            "True Label: neutral\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 8:\n",
            "Premise: A female softball player wearing blue and red crouches in the infield, waiting for the next play.\n",
            "Hypothesis: the player is flying planes\n",
            "True Label: contradiction\n",
            "Model Prediction: contradiction\n",
            "\n",
            "Example 9:\n",
            "Premise: Workers standing on a lift.\n",
            "Hypothesis: Workers stand on a lift\n",
            "True Label: entailment\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 10:\n",
            "Premise: Two men in neon yellow shirts busily sawing a log in half.\n",
            "Hypothesis: Two men are cutting wood to build a table.\n",
            "True Label: neutral\n",
            "Model Prediction: entailment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# Define paths and device\n",
        "base_model_path = \"microsoft/phi-2\"  # Path to the base model\n",
        "adapter_model_path = \"./snli_finetune_phi2/final_model\"  # Path to the fine-tuned LoRA adapter\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set up quantization configuration to reduce memory usage\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load the base model with quantization and tokenizer\n",
        "base_model = AutoModelForCausalLM.from_pretrained(base_model_path, quantization_config=bnb_config).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_path, model_max_length=512, padding_side=\"left\", add_eos_token=True)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load the LoRA adapter model on top of the base model\n",
        "model = PeftModel.from_pretrained(base_model, adapter_model_path).to(device)\n",
        "\n",
        "# Define label map and evaluation function\n",
        "label_map = {0: \"entailment\", 1: \"neutral\", 2: \"contradiction\"}\n",
        "\n",
        "# Improved evaluation function using regex and accuracy_score\n",
        "def evaluate_model(model, tokenizer, dataset, max_length=70):\n",
        "    print(f\"Dataset length: {len(dataset)}\")\n",
        "    model.eval()\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    for i in tqdm(range(len(dataset)), position=0, leave=True, desc=\"Predicting on test Samples\"):\n",
        "        premise = dataset['premise'][i]\n",
        "        hypothesis = dataset['hypothesis'][i]\n",
        "        label = dataset['label'][i]\n",
        "\n",
        "        # Skip ambiguous label (-1) in SNLI dataset\n",
        "        if label == -1:\n",
        "            print(f\"Skipped example {i} (ambiguous label)\")\n",
        "            continue\n",
        "        true_labels.append(label_map[label])\n",
        "\n",
        "        # Concatenate premise and hypothesis with a specific prompt\n",
        "        input_text = (\n",
        "            f\"Premise: {premise}\\n\"\n",
        "            f\"Hypothesis: {hypothesis}\\n\"\n",
        "            f\"Answer with one of the following: entailment, neutral, contradiction.\\nAnswer:\"\n",
        "        )\n",
        "\n",
        "        # Tokenize input text\n",
        "        inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(model.device)\n",
        "\n",
        "        # Generate prediction\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_new_tokens=4)  # Strict limit on max_new_tokens to get concise answers\n",
        "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "\n",
        "            # Use regex to extract the label directly after \"Answer:\"\n",
        "            match = re.search(r\"Answer:\\s*(entailment|neutral|contradiction)\", prediction, re.IGNORECASE)\n",
        "            if match:\n",
        "                prediction = match.group(1).lower()\n",
        "            else:\n",
        "                prediction = \"neutral\"  # Default if no match is found\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    return accuracy, predictions\n",
        "\n",
        "# Ensure pad_token_id is set\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load the SNLI test dataset\n",
        "from datasets import load_dataset\n",
        "snli_dataset = load_dataset('snli')\n",
        "test_dataset = snli_dataset['test'].select([i for i in range(0, 10000, 100)][:100])\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "fine_tuned_accuracy, predictions = evaluate_model(model, tokenizer, test_dataset)\n",
        "\n",
        "# Print the accuracy in the specified format\n",
        "print(f\"\\nFine-tuned Model Accuracy: {fine_tuned_accuracy * 100:.2f}%\")\n",
        "print(\"\\nDetailed Example Outputs after fine-tuning:\")\n",
        "\n",
        "# Display 10 examples with Premise, Hypothesis, True Label, and Model Prediction\n",
        "for i in range(10):  # Displaying 10 examples for more detailed inspection\n",
        "    premise = test_dataset['premise'][i]\n",
        "    hypothesis = test_dataset['hypothesis'][i]\n",
        "    true_label = label_map[test_dataset['label'][i]]\n",
        "    predicted_label = predictions[i]\n",
        "\n",
        "    print(f\"\\nExample {i + 1}:\")\n",
        "    print(f\"Premise: {premise}\")\n",
        "    print(f\"Hypothesis: {hypothesis}\")\n",
        "    print(f\"True Label: {true_label}\")\n",
        "    print(f\"Model Prediction: {predicted_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fMBmO7mpBW5",
        "outputId": "b6a43a38-7c6a-494d-a891-ba29d38b527e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fine-tuned Model Accuracy: 72.0%\n",
            "\n",
            "Detailed Example Outputs after fine-tuning:\n",
            "\n",
            "Example 1:\n",
            "Premise: This church choir sings to the masses as they sing joyous songs from the book at a church.\n",
            "Hypothesis: The church has cracks in the ceiling.\n",
            "True Label: neutral\n",
            "Model Prediction: neutral\n",
            "\n",
            "Example 2:\n",
            "Premise: A woman within an orchestra is playing a violin.\n",
            "Hypothesis: A woman is playing the violin.\n",
            "True Label: entailment\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 3:\n",
            "Premise: Two men climbing on a wooden scaffold.\n",
            "Hypothesis: Two sad men climbing on a wooden scaffold.\n",
            "True Label: neutral\n",
            "Model Prediction: contradiction\n",
            "\n",
            "Example 4:\n",
            "Premise: A man in a black shirt, in a commercial kitchen, holding up meat he took out of a bag.\n",
            "Hypothesis: A man in a black shirt, in a commercial kitchen, holding up the old meat he took out of a bag.\n",
            "True Label: neutral\n",
            "Model Prediction: neutral\n",
            "\n",
            "Example 5:\n",
            "Premise: A woman in a black shirt looking at a bicycle.\n",
            "Hypothesis: A woman dressed in black shops for a bicycle.\n",
            "True Label: neutral\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 6:\n",
            "Premise: many children play in the water.\n",
            "Hypothesis: The children are playing mini golf.\n",
            "True Label: contradiction\n",
            "Model Prediction: contradiction\n",
            "\n",
            "Example 7:\n",
            "Premise: A group of people stand near and on a large black square on the ground with some yellow writing on it.\n",
            "Hypothesis: a group of people wait\n",
            "True Label: neutral\n",
            "Model Prediction: neutral\n",
            "\n",
            "Example 8:\n",
            "Premise: A female softball player wearing blue and red crouches in the infield, waiting for the next play.\n",
            "Hypothesis: the player is flying planes\n",
            "True Label: contradiction\n",
            "Model Prediction: contradiction\n",
            "\n",
            "Example 9:\n",
            "Premise: Workers standing on a lift.\n",
            "Hypothesis: Workers stand on a lift\n",
            "True Label: entailment\n",
            "Model Prediction: entailment\n",
            "\n",
            "Example 10:\n",
            "Premise: Two men in neon yellow shirts busily sawing a log in half.\n",
            "Hypothesis: Two men are cutting wood to build a table.\n",
            "True Label: neutral\n",
            "Model Prediction: neutral\n"
          ]
        }
      ]
    }
  ]
}